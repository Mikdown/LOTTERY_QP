{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as npy\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "import datetime\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('assets/lottery_data.db')\n",
    "\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The following two code cells meet the first feature requirements:\n",
    "1. Read two data files (CSV). \n",
    "    - Four CSV files are read in and cleaned (transformed). \n",
    "2. Set up a local database and read data in with SQLite. \n",
    "    - After transformation the files are read into a SQLite DB to be used for futher analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      draw_date           game  num_1  num_2  num_3  num_4  num_5  mb     month day_name  day_num  day_date draw_time\n",
      "0    2003-12-05  Mega Millions     12     44     15     18      1  42  December   Friday        4         5  23:00:00\n",
      "1    2003-12-09  Mega Millions     14     15     48      4     24  41  December  Tuesday        1         9  23:00:00\n",
      "2    2003-12-12  Mega Millions     16     32     46      9     45  26  December   Friday        4        12  23:00:00\n",
      "3    2003-12-16  Mega Millions     47     16     31     24     46  47  December  Tuesday        1        16  23:00:00\n",
      "4    2003-12-19  Mega Millions      5     10     39     17     35  38  December   Friday        4        19  23:00:00\n",
      "...         ...            ...    ...    ...    ...    ...    ...  ..       ...      ...      ...       ...       ...\n",
      "2006 2023-02-24  Mega Millions     22     49      2     65     67   7  February   Friday        4        24  23:00:00\n",
      "2007 2023-02-28  Mega Millions     59     52     40     14     16  13  February  Tuesday        1        28  23:00:00\n",
      "2008 2023-03-03  Mega Millions     39      8     67     36     25  11     March   Friday        4         3  23:00:00\n",
      "2009 2023-03-07  Mega Millions     15     69     28     25     22  21     March  Tuesday        1         7  23:00:00\n",
      "2010 2023-03-10  Mega Millions     60      9     20     59     63   5     March   Friday        4        10  23:00:00\n",
      "\n",
      "[2011 rows x 13 columns]\n",
      "                  DATE_TIME DPTemp  DBTemp  Precip  RHumid        Date      Time\n",
      "0       2013-01-01T00:53:00     36      47     0.0      66  2013-01-01  00:53:00\n",
      "1       2013-01-01T01:53:00     36      43     0.0      76  2013-01-01  01:53:00\n",
      "2       2013-01-01T02:51:00     37      45     0.0      76  2013-01-01  02:51:00\n",
      "3       2013-01-01T02:53:00     37      44     0.0      76  2013-01-01  02:53:00\n",
      "4       2013-01-01T03:53:00     38      45     0.0      77  2013-01-01  03:53:00\n",
      "...                     ...    ...     ...     ...     ...         ...       ...\n",
      "115441  2022-12-16T22:53:00     28      43     0.0      56  2022-12-16  22:53:00\n",
      "115442  2022-12-16T23:53:00     27      44     0.0      51  2022-12-16  23:53:00\n",
      "115443  2022-12-16T23:59:00    NaN       0     0.0       0  2022-12-16  23:59:00\n",
      "115444  2022-12-17T00:53:00     28      45     0.0      52  2022-12-17  00:53:00\n",
      "115445  2022-12-17T01:53:00     28      45     0.0      52  2022-12-17  01:53:00\n",
      "\n",
      "[115446 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "mega_draw_file = (\"assets/megamillions.csv\")\n",
    "mega_weather_file = (\"assets/mega_weather.csv\")\n",
    "\n",
    "\n",
    "mega_draw_df = pd.read_csv(mega_draw_file, engine='python', parse_dates= {\"draw_date\" : [\"year\",\"month\",\"day\"]})\n",
    "mega_weather_df = pd.read_csv(mega_weather_file, engine='python')\n",
    "\n",
    "#Split the \"DATE_TIME\" column into 2 seperate columns \"Date\" and \"Time\".\n",
    "\n",
    "mega_weather_df['Date'] = pd.to_datetime(mega_weather_df['DATE_TIME']).dt.date\n",
    "mega_weather_df['Time'] = pd.to_datetime(mega_weather_df['DATE_TIME']).dt.time\n",
    "\n",
    "pd.to_datetime(mega_draw_df['draw_date'], errors='coerce')\n",
    "mega_draw_df['month'] = pd.DatetimeIndex(mega_draw_df['draw_date']).month_name()\n",
    "mega_draw_df['day_name'] = pd.DatetimeIndex(mega_draw_df['draw_date']).day_name()\n",
    "mega_draw_df['day_num'] = pd.DatetimeIndex(mega_draw_df['draw_date']).dayofweek\n",
    "mega_draw_df['day_date'] = pd.DatetimeIndex(mega_draw_df['draw_date']).day\n",
    "mega_draw_df['draw_time'] = '23:00:00'\n",
    "\n",
    "#mega_draw_df.to_sql('mb_draw', conn_mb, if_exists='append', index = False)\n",
    "#mega_weather_df.to_sql('mb_weather', conn_mb, if_exists='append', index = False)\n",
    "\n",
    "print(mega_draw_df)\n",
    "print(mega_weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date_Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date_Time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pb_weather_df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(pb_weather_df[\u001b[39m'\u001b[39m\u001b[39mDATE_TIME\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mdate\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pb_weather_df[\u001b[39m'\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(pb_weather_df[\u001b[39m'\u001b[39m\u001b[39mDATE_TIME\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mtime\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pb_weather_df[\u001b[39m\"\u001b[39;49m\u001b[39mDate_Time\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mstrftime(date_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pd\u001b[39m.\u001b[39mto_datetime(pb_draw_df[\u001b[39m'\u001b[39m\u001b[39mdraw_date\u001b[39m\u001b[39m'\u001b[39m], errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoerce\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/miked/Documents/Code_Louisville_Project/Lottery-Picker/MegaMil.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pb_draw_df[\u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDatetimeIndex(pb_draw_df[\u001b[39m'\u001b[39m\u001b[39mdraw_date\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mmonth_name()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date_Time'"
     ]
    }
   ],
   "source": [
    "pb_draw_file = (\"assets/powerball.csv\")\n",
    "pb_weather_file = (\"assets/pb_weather.csv\")\n",
    "\n",
    "pb_draw_df = pd.read_csv(pb_draw_file, engine='python', parse_dates= {\"draw_date\" : [\"year\",\"month\",\"day\"]})\n",
    "pb_weather_df = pd.read_csv(pb_weather_file, engine='python')\n",
    "\n",
    "pb_weather_df['Date'] = pd.to_datetime(pb_weather_df['DATE_TIME']).dt.date\n",
    "pb_weather_df['Time'] = pd.to_datetime(pb_weather_df['DATE_TIME']).dt.time\n",
    "\n",
    "pd.to_datetime(pb_draw_df['draw_date'], errors='coerce')\n",
    "pb_draw_df['month'] = pd.DatetimeIndex(pb_draw_df['draw_date']).month_name()\n",
    "pb_draw_df['day_name'] = pd.DatetimeIndex(pb_draw_df['draw_date']).day_name()\n",
    "pb_draw_df['day_num'] = pd.DatetimeIndex(pb_draw_df['draw_date']).dayofweek\n",
    "pb_draw_df['day_date'] = pd.DatetimeIndex(pb_draw_df['draw_date']).day\n",
    "pb_draw_df['draw_time'] = '2300'\n",
    "\n",
    "#pb_draw_df.to_sql('pb_draw', conn_pb, if_exists='append', index = False)\n",
    "#pb_weather_df.to_sql('pb_weather', conn_pb, if_exists='append', index = False)\n",
    "\n",
    "print(pb_draw_df)\n",
    "print(pb_weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_df = weather_df.astype({\"HourlyRelativeHumidity\": float})\n",
    "weather_filter_df = mega_weather_df.query('DBTemp == 42 & RHumid == 73')\n",
    "#weather_df.dtypes\n",
    "#weather_filter_df\n",
    "weather_date_unique = weather_filter_df['Date'].unique()\n",
    "weather_date_unique_df = pd.DataFrame(weather_date_unique, columns=['date_time'])\n",
    "weather_date_unique_df['date_time'] = pd.to_datetime(weather_date_unique_df['date_time'])\n",
    "\n",
    "weather_date_unique_df['time_only'] = pd.to_datetime(weather_date_unique_df['date_time']).dt.time\n",
    "weather_date_unique_df['draw_date'] = pd.to_datetime(weather_date_unique_df['date_time']).dt.date\n",
    "weather_date_unique_df['draw_date'] = pd.to_datetime(weather_date_unique_df['draw_date'])\n",
    "weather_date_unique_df['draw_date'] = weather_date_unique_df['draw_date'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "\n",
    "dates_list = weather_date_unique_df['draw_date'].values.tolist()\n",
    "\n",
    "#with pd.option_context('display.max_rows', None,\n",
    "#                       'display.max_columns', None,\n",
    "#                       'display.precision', 3,\n",
    "#                       ):\n",
    "#    print(weather_date_unique_df)\n",
    "dates_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = mega_draw_df.iloc[:, 2:7].values\n",
    "\n",
    "mega_draw_df['check'] = npy.any(((arr[:, 1:] - arr[:, :-1]) == 1),axis=1).astype(int)\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(mega_draw_df)\n",
    "\n",
    "mega_draw_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1 = mega_draw_df['num_1'].value_counts()\n",
    "counts_2 = mega_draw_df['num_2'].value_counts()\n",
    "counts_3 = mega_draw_df['num_3'].value_counts()\n",
    "counts_4 = mega_draw_df['num_4'].value_counts()\n",
    "counts_5 = mega_draw_df['num_5'].value_counts()\n",
    "counts_MB = mega_draw_df['MB'].value_counts()\n",
    "#counts_df.columns = ['num_1', 'counts_1']\n",
    "#with pd.option_context('display.max_rows', None,\n",
    "                       #'display.max_columns', None,\n",
    "                       #'display.precision', 3,\n",
    "                       #):\n",
    "    #print(counts_1, counts_2)\n",
    "counts_df = pd.concat([counts_1, counts_2, counts_3, counts_4, counts_5, counts_MB], axis=1)\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_draw_df[0:2][['num_1', 'num_5']] #Slice the dataframe by selected row indexes and a list of column names.\n",
    "mega_draw_df['num_3'][:4] #create a series from the first 4 rows (0 - 3) of the column string \"num_3\".\n",
    "dft['num_2' : 'num_5'] #Slice transposed dataframe with row name from 'num_2 up to and including 'num_5'.\n",
    "dft.loc['num_3', 2] #Row/Column based indexing. loc uses lables to slice.\n",
    "mega_draw_df.iloc[1:100, :] #Slice dataframe by rows 1 - 99 and all columns. iloc uses integers as index positions to slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_draw_df['draw_date'] = pd.to_datetime(mega_draw_df['draw_date'])\n",
    "date_df = mega_draw_df[mega_draw_df['draw_date'].dt.month == 12]\n",
    "\n",
    "counts_1 = date_df['num_1'].value_counts()\n",
    "counts_2 = date_df['num_2'].value_counts()\n",
    "counts_3 = date_df['num_3'].value_counts()\n",
    "counts_4 = date_df['num_4'].value_counts()\n",
    "counts_5 = date_df['num_5'].value_counts()\n",
    "counts_MB = date_df['MB'].value_counts()\n",
    "counts_df = pd.concat([counts_1, counts_2, counts_3, counts_4, counts_5, counts_MB], axis=1)\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(counts_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_draw_df['draw_date'].astype(str)\n",
    "df2 = mega_draw_df[mega_draw_df['draw_date'].isin(dates_list)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1 = df2['num_1'].value_counts()\n",
    "counts_2 = df2['num_2'].value_counts()\n",
    "counts_3 = df2['num_3'].value_counts()\n",
    "counts_4 = df2['num_4'].value_counts()\n",
    "counts_5 = df2['num_5'].value_counts()\n",
    "counts_MB = df2['MB'].value_counts()\n",
    "counts_MB_df= pd.DataFrame(counts_MB)\n",
    "sorted_MB = counts_MB_df.sort_values('MB', ascending=False)\n",
    "counts_df = pd.concat([counts_1, counts_2, counts_3, counts_4, counts_5], axis=1)\n",
    "counts_df['totals'] = counts_df.sum(axis=1, skipna=True)\n",
    "sorted_df = counts_df.sort_values('totals', ascending=False)\n",
    "\n",
    "with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(sorted_df)\n",
    "    print(sorted_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_list=(sorted_df.index.values)\n",
    "draw_list_MB=(sorted_MB.index.values)\n",
    "print(draw_list)\n",
    "print(draw_list_MB)\n",
    "type(draw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_list = list(draw_list)\n",
    "new_list1 = random.sample(draw_list, 5)\n",
    "new_list1 = sorted(new_list1)\n",
    "draw_list_MB = list(draw_list_MB)\n",
    "new_list_MB = random.sample(draw_list_MB, 1)\n",
    "#sorted([str(x) for x in new_list1])\n",
    "print(new_list1 + new_list_MB)\n",
    "type(new_list1[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
